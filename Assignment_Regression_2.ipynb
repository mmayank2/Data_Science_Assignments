{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0e12d6-ec4f-4a11-be2f-b7de42df5a33",
   "metadata": {},
   "source": [
    "# Regression Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82178710-cd38-4c41-a281-271c70203cb3",
   "metadata": {},
   "source": [
    "# Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00367296-da4e-4f08-be3e-537a0b0d8a4c",
   "metadata": {},
   "source": [
    " <font color=\"red\"> Answer :-</font> R-squared or coefficient of determination is like a report card for the Linear Regression model.it tell how well the model prediction match the actual value means goodness of fit. it ranges from 0 to 1. A zero show model does not explain the variability in the dependent variable and 1 show model explain all the variability of dependent variable. The higher the r-squared better model is explained."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e6228b4-39b8-429f-b6c0-48143ba8ff38",
   "metadata": {},
   "source": [
    "R-squared=1-(SSR/SST)\n",
    "Here SSR is sum of square of residual\n",
    "and SST is the sum of square of total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf0e29-9504-4797-8cde-4807e7396c11",
   "metadata": {},
   "source": [
    "a. R-squared ranges from 0 to 1. A value of 0 indicates that the model does not explain any variability in the dependent variable, while a value of 1 indicates that the model explains all the variability.<br>\n",
    "b. Higher R-squared values suggest a better fit of the model to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ac84d-57d1-4469-a185-71c65438406b",
   "metadata": {},
   "source": [
    "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a68a31-0805-4ec5-b740-9eaa4c82c4d2",
   "metadata": {},
   "source": [
    " <font color=\"red\"> Answer :-</font> Adjusted R-Squared is the modified version of the R-squared. While R-squared tells us how well our model predicts where as Adjusted r_squared also consider the the number of predictors (features or independent variable)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cac4e99c-c24e-451d-86e1-186bb91d0d3b",
   "metadata": {},
   "source": [
    "adjusted r-squared= 1- ((1-r2)(n-1))/(n-k-1)\n",
    "where r2 is r-squared\n",
    "n= number of observation\n",
    "k=number of features or predictors"
   ]
  },
  {
   "cell_type": "raw",
   "id": "652f9396-728f-45ae-a502-fc61c29b070a",
   "metadata": {},
   "source": [
    "There are some difference between the R-squared and Adjusted R-squared:-\n",
    "1. Role:- i. R-squared is used for knowing the performace of the model\n",
    "          ii. Adjusted r-squared is used for knowing the performance as well as the relevalent features.\n",
    "2. Focus :- i. R-squared is how well the model predictions match the actual value.\n",
    "            ii. Adjusted r-squared consider wellness of model prediction as well as number of predictors.\n",
    "Adjusted r-squared penalizes the unnecessary predictors but r-squared doesn't.\n",
    "mainly r-squared is used for quick overview of model performance and Adjusted r-squared is used when we deal with multiple number of predictors or features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40034400-eb9e-4f4b-b2fd-7742bcf983d8",
   "metadata": {},
   "source": [
    "# Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf36b56-aa5e-49dd-b7c0-ab0c56711849",
   "metadata": {},
   "source": [
    "Multiple Predictors:\n",
    "\n",
    "When dealing with models that have multiple predictors. Regular R-squared may give too much credit to models with more predictors, even if those predictors don't significantly improve the model. Adjusted R-squared corrects for this by penalizing unnecessary predictors.\n",
    "Model Comparison:\n",
    "\n",
    "When comparing models with different numbers of predictors. If you're considering different models, some with more predictors than others, Adjusted R-squared provides a fair comparison by adjusting for the number of predictors. It helps you assess whether the improvement in fit is worth the additional complexity.\n",
    "Preventing Overfitting:\n",
    "\n",
    "When preventing overfitting is a concern. Overfitting occurs when a model is too complex and fits the training data too closely, potentially leading to poor generalization to new data. Adjusted R-squared helps prevent overfitting by penalizing models that are overly complex without substantial gains in explanatory power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8fef58-8bf2-48d9-a527-fd4a3e78e160",
   "metadata": {},
   "source": [
    "<font color=\"Red\">Answer :- </font> Adjusted r-squared is more appropriate for the following situation:-\n",
    "1. Multiple Predictors :- when dealing with models have multiple predictors. Regular R-squared may gives too much credit to model with more predictors even if the predictor is not significant for the model.Addjusted r-squared is correct for this situation it penalised the unnecessary features or predictors. \n",
    "2. Model Comparison :- adjusted r squared provide fair comparison by adjusting the number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f59a054-6201-41fa-a36f-c5924c63bf1d",
   "metadata": {},
   "source": [
    "# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18a025-793c-4067-85df-990c1533f1a1",
   "metadata": {},
   "source": [
    "<font color=\"red\">Answer :- </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d9ade-86c9-46f2-accd-3b49110481ba",
   "metadata": {},
   "source": [
    "MSE:- Mean squared error is define as the sum of square of the difference between the actual and predicted value divided by the total number of data points. <br>\n",
    "\n",
    "Formula:- summation symbol i=1 to n ((actual point-predicted point)**2)/ n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e02d13be-cf19-4d1a-8001-4abc217b70fb",
   "metadata": {},
   "source": [
    "Advantage:- 1. One local and global minima \n",
    "            2. Loss function (differential at every point)\n",
    "            3. Converges faster\n",
    "Disadvantage:- 1. Unit changed\n",
    "               2. Not Robust to outlier"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b55fe323-57c3-4e9e-9939-ccfd3782806b",
   "metadata": {},
   "source": [
    "MAE :- Mean Absoulute Error is define as the sum of the difference between the actual and predicted value divided by the total number of data points.\n",
    "\n",
    "Formula:- summation symbol i=1 to n |actual point - predicted point|/n\n",
    "Advantage:- 1. Same unit \n",
    "            2. Robust to outliers\n",
    "Disadvantage:- 1. cannot differentiable at 0\n",
    "               2. convergence take more time "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a70665d0-80f2-4722-9c3f-d4cc0026bfa3",
   "metadata": {},
   "source": [
    "RMSA :- Root mean squared Error is define as root of mean squared error.\n",
    "\n",
    "Formula :- RSMA = (MSA)**0.5\n",
    "\n",
    "Advantage:- 1. Same unit\n",
    "            2. Differentiable\n",
    "Disadvantage:- 1. Not Robust to outlier"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94babeb3-7d12-4ae4-92e9-14a25a539b34",
   "metadata": {},
   "source": [
    "MSE and RMSE:\n",
    "These metrics are sensitive to large errors and are commonly used when larger errors should be penalized more in the evaluation.\n",
    "Suitable when the dataset contains outliers or when precise prediction accuracy is critical.\n",
    "\n",
    "MAE:\n",
    "MAE is less sensitive to outliers and provides a more balanced view of overall model performance.\n",
    "Suitable when the emphasis is on understanding the average magnitude of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce70633-e65d-4936-9648-ab742ebe2443",
   "metadata": {},
   "source": [
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0176378-a016-4c43-b54d-efb2b6580a93",
   "metadata": {},
   "source": [
    "MSE:-Advantage:- 1. One local and global minima \n",
    "            2. Loss function (differential at every point)\n",
    "            3. Converges faster\n",
    "Disadvantage:- 1. Unit changed\n",
    "               2. Not Robust to outlier\n",
    "    \n",
    "MAE :- Advantage:- 1. Same unit \n",
    "            2. Robust to outliers\n",
    "Disadvantage:- 1. cannot differentiable at 0\n",
    "               2. convergence take more time \n",
    "\n",
    "RMSE :- Advantage:- 1. Same unit\n",
    "            2. Differentiable\n",
    "Disadvantage:- 1. Not Robust to outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6afbcfd-7fb3-49c9-a1d5-6394bbb25c23",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada524d4-13f8-464e-981f-3cad2835b616",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Answer : </font> Lasso regression, commonly referred to as L1 regularization, is a method for stopping overfitting in linear regression models by including a penalty term in the cost function. In contrast to Ridge regression, it adds the total of the absolute values of the coefficients rather than the sum of the squared coefficients.\n",
    "Lasso regression can reduce certain coefficients to zero, conducting feature selection in effect. With high-dimensional datasets where many characteristics could be unnecessary or redundant, this is very helpful."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f146d159-dd54-47cd-a5a5-d07c1c155473",
   "metadata": {},
   "source": [
    "Ridge Regression :- \n",
    "a. shrink the cofficients toward zero but not exactly zero.\n",
    "b. Adds a penalty term proportional to the sum of Squared values of coefficients\n",
    "c. Does not eliminate any features\n",
    "d. Suitable when all features are importantly\n",
    "e. L2 Regularization adds the square of the coefficient to the loss function\n",
    "d. Performs better when there are many small to medium-sized coefficients\n",
    "\n",
    "Lasso regularization :-\n",
    "a. Encourages some coefficients to be exactly zero.\n",
    "b. Adds a penalty term proportional to the sum of absolute values of coefficients\n",
    "c. it Can eliminate some features\n",
    "d. Suitable when some features are irrelevant or redundant\n",
    "e. Performs better when there are a few large coefficients"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1f91d39-bde0-4800-837f-6412aca1a08a",
   "metadata": {},
   "source": [
    "When to Use:\n",
    "Lasso:\n",
    "a. When you suspect that many features are irrelevant or redundant.\n",
    "b. When you want a sparse model with fewer variables.\n",
    "c. For feature selection in high-dimensional datasets.\n",
    "\n",
    "Ridge:\n",
    "a. When all features are potentially relevant, and you don't want to exclude any entirely.\n",
    "b. When there is multicollinearity among features (correlation between predictors).\n",
    "c. For improved stability in the presence of highly correlated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464257d-8384-4b24-8d93-f0bcdb602f7d",
   "metadata": {},
   "source": [
    "# Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7528f6-54a9-46a2-af95-fe5b673e832a",
   "metadata": {},
   "source": [
    "<font color=\"red\">Answer :- </font> Regularized linear models help prevent overfitting in machine learning by discouraging overly complex models with large coefficients. This is achieved by adding a penalty term to the cost function during training, which limits the flexibility of the model. By doing so, the model becomes less likely to fit noise in the training data and performs better when faced with new, unseen data.\n",
    "\n",
    "For example, let's say you're trying to predict house prices based on various features like size, number of bedrooms, and location. Without regularization, the model might become too complex, fitting the training data too closely, even capturing random fluctuations or noise. As a result, it may perform poorly when trying to predict the prices of new houses it hasn't seen before.\n",
    "\n",
    "By applying regularization, the model is encouraged to keep the coefficients of the features smaller, effectively simplifying the model. This helps prevent overfitting by ensuring that the model doesn't rely too heavily on any particular feature or noise in the data. As a result, the model becomes more robust and performs better on unseen data, providing more reliable predictions of house prices in different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96614666-6cca-40b5-8a7e-242bce21f0da",
   "metadata": {},
   "source": [
    "# Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0b5ba-a043-4c0f-a6e1-82d32985774e",
   "metadata": {},
   "source": [
    "<font color=\"red\">Answer :- </font> While regularized linear models offer valuable tools for mitigating overfitting and improving generalization performance in regression analysis, they also come with certain limitations that may make them less suitable in certain situations:\n",
    "\n",
    "1. Sensitivity to Hyperparameters: Regularized linear models typically involve hyperparameters, such as the regularization strength (e.g., lambda for Ridge and alpha for Lasso). Tuning these hyperparameters effectively requires cross-validation or other techniques, which can be computationally expensive and may not always yield optimal results.\n",
    "\n",
    "2. Assumption of Linearity: Linear models assume a linear relationship between the input features and the target variable. However, in real-world scenarios, the relationship may be more complex and nonlinear. In such cases, linear models may fail to capture the underlying patterns in the data, leading to suboptimal performance.\n",
    "\n",
    "3. Limited Expressiveness: Regularized linear models are inherently limited in their expressiveness compared to more complex models such as decision trees, neural networks, or ensemble methods. They may struggle to capture intricate interactions and nonlinearities present in the data, especially when dealing with high-dimensional or highly nonlinear datasets.\n",
    "\n",
    "4. Feature Scaling Sensitivity: Regularized linear models are sensitive to the scale of the input features. If the features are not properly scaled (e.g., one feature has a much larger scale than others), it can affect the performance of the model and the interpretation of the coefficients.\n",
    "\n",
    "5. Feature Selection Bias: While Lasso regularization can induce sparsity by driving some coefficients to zero, it may also introduce bias in feature selection, potentially excluding relevant features from the model. This can lead to underfitting and loss of important information, particularly if the true underlying model is complex.\n",
    "\n",
    "6. Limited Interpretability in High Dimensions: In high-dimensional datasets where the number of features is large relative to the number of samples, interpreting the coefficients of regularized linear models becomes challenging. It may be difficult to discern the relative importance of individual features and understand the model's decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218156b-2770-4691-8d9f-504d18d509e6",
   "metadata": {},
   "source": [
    "# Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3573fe-917c-43e9-877e-6502e7499ad1",
   "metadata": {},
   "source": [
    "<font color=\"red\">Answer :- </font> if minimizing the average absolute error is the primary concern and outliers should not be heavily penalized, Model B (with the lower MAE) would be the preferred choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36093f74-0723-455d-88f2-9389a002c158",
   "metadata": {},
   "source": [
    "Limitation :- Both RMSE and MAE have limitations. For example, they don't provide information about the direction of errors (overestimation vs. underestimation).\n",
    "The choice of metric depends on the specific context of the problem and the preferences of the stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b1834-f87b-40a7-a678-395e451d2b2f",
   "metadata": {},
   "source": [
    "# Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7583344c-17d7-47fa-84db-1f6d62767e20",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Answer :- </font> Model A (Ridge regularization with λ=0.1) imposes a moderate penalty on the magnitude of coefficients, allowing for some degree of shrinkage towards zero.\n",
    "Model B (Lasso regularization with λ=0.5) imposes a stronger penalty, potentially leading to more coefficients being exactly zero and thus a sparser model.\n",
    "To decide which model is better, you would typically evaluate their performance using appropriate validation techniques such as cross-validation and comparing metrics like mean squared error (MSE) or mean absolute error (MAE) on a hold-out dataset.\n",
    "\n",
    "Trade-offs and Limitations of Regularization Methods:\n",
    "\n",
    "Ridge Regularization: Ridge regularization tends to perform well when the dataset contains highly correlated features. However, it may not perform as well in situations where feature selection is crucial, as it doesn't automatically eliminate irrelevant features.\n",
    "\n",
    "Lasso Regularization: Lasso regularization is effective at feature selection by setting some coefficients to exactly zero. However, it may struggle with multicollinear features since it tends to arbitrarily select one feature over others, and the selected features may not always be consistent across different runs or datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729945fc-0482-429b-b153-ac066c05a4d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb30c5-bfee-4ebd-9a83-ff331b796de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d8ef8-1b89-4094-b448-5ab1e4daf9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce4a5c5-d98c-40a7-868e-9a4887a22553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513885c9-1c52-4ef4-b933-510eb7ac6b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3460c86-139d-4b8b-89b8-10dd6d731906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756638d9-e311-40fe-8574-d5f73e66ea20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a123e2-370f-4333-b0ae-229ed23b6d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d4699-e03b-4d1d-a0d6-572a126f1abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775afe45-1e95-429e-9b87-f0703315d031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea158d8-1d73-4066-a214-a2853a799d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d06a50-fdc6-49a1-8dbb-b96b17e66ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e88d9d-a219-47f4-8720-c062973a51f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308135fc-3ee2-443b-a270-b52309de92cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcd55331-d49c-4b78-8734-75330d30bfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c567e-fe92-4b9a-ae9d-163253d44010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc93799-1b26-46aa-a808-fb1710722b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794e622-7dbb-4411-9ebe-976bdeb35519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
