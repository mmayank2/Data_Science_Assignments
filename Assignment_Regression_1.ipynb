{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be1cca2-71b6-49f2-9eba-6ac19ddfb5cd",
   "metadata": {},
   "source": [
    "# <font color=\"Red\"> <u> Assignment 26th March Regression - 1 </u></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d4f43-915c-41d4-890c-c13155d9abce",
   "metadata": {},
   "source": [
    "<font color=\"red\">Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f95743-3f2f-46bd-9cc5-ca94a7b24f8c",
   "metadata": {},
   "source": [
    "Simple Linear Regression :- \n",
    "1. Simple Linear Regression uses only single independent variable (predictor) to predict dependent variable (target)\n",
    "2. Simple and easy to implement\n",
    "3. it represent straight line relationship between independent variable and dependent variable.\n",
    "4. it aims to find best fitting line that minimize the sum of squared error.\n",
    "\n",
    "Example :- Suppose we have to predict exam score  based on number of hours they studied.\n",
    "\n",
    "\n",
    "Multiple Linear Regression :-\n",
    "1. Multiple Linear Regression uses multiple independent variable (Predictor) to predict dependent variable (target)\n",
    "2. it is more complex because it uses multiple predictor.\n",
    "3. it represent hyperplane relationship between independent variables and dependent variables.\n",
    "4. it aims to find best fit hyperplane that minimize the sum of error.\n",
    "\n",
    "Example :- Suppose we have to predict exam score based on number of hour they studied and their IQ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d0683e-ef00-465d-b6bc-9d18e91f30cc",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ee7a1-c41e-4a80-b808-0fa5584251bb",
   "metadata": {},
   "source": [
    "Assumption:-\n",
    "1. Linear Relation between inputs and output features.\n",
    "2. No Multicollinearity\n",
    "3. Normality of Residual\n",
    "4. Homoscedasticity \n",
    "5. No Autocorrection of error "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916d30a-cd60-4830-8610-c94184821fc4",
   "metadata": {},
   "source": [
    "1. Linear Relationship between inputs and output features :- \n",
    "    <br>By using scatterplot of each individual variable with dependend variable.\n",
    "    \n",
    "2. No Multicollinearity :- we try to check correlation between all input columns. if it highly correlated then it not satisfy the assumption.\n",
    "    <br>By using variance_inflation_factor\n",
    "    and By using heatmap \n",
    "    \n",
    "3. Normality of Residual :- we check the distribution of residual \n",
    "    <br> By using kde plot or Q-Q plot\n",
    "    \n",
    "4. Homoscedasticity :- we check the distribution of residual and y_pred.\n",
    "\n",
    "5. No Auto correlation of Error:- Here we check by ploting Residual value that it makes pattern or not if it make pattern then it not satisfied the assumption.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f67ce-3cfa-4ca1-a6f8-8d9320df36c2",
   "metadata": {},
   "source": [
    "<font color=\"red\"> Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using \n",
    "a real-world scenario. </font>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20535aff-b8f1-48df-93df-417735a507c4",
   "metadata": {},
   "source": [
    "In Linear Regression model ,\n",
    "y=mx+b\n",
    "where y represent the dependent variable \n",
    "x represent the independent variable \n",
    "m represent the slope of the line which represents the change in y for one unit change in x\n",
    "b represent the intercept, which represent the value of y when x=0.\n",
    "\n",
    "Interpretation of the slope(m):\n",
    "    m>0 ( Positive slop ) indicate the positive correlation between the dependent and independent variable. As x increase then y also be increased.\n",
    "    m<0 (Negative slop ) indicate the negative relationship between the predictor and target variable. As x increase then y decrease.\n",
    "    \n",
    "Interpretation of the intercept(b):\n",
    "    the intercept represent the value of y when x is zero. \n",
    "    \n",
    "Example:- Lets say we're trying to predict marks based on study time per day.\n",
    "    y represent the marks\n",
    "    and x represent the study time .\n",
    "    our Linear Regression model might look like:-\n",
    "    marks = m * (study_time) + b\n",
    "    marks =5 * (study_time) + 10\n",
    "    m :- Represent the rate of change in marks for every one unit change in study time. In this slope is 5 which represent every unit change in study time,marks increased by 5 marks. \n",
    "    b:- the intercept represent the marks obtain when study time is zero.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129534c5-2e5d-419c-b7ca-4509860b8b41",
   "metadata": {},
   "source": [
    "<font color=\"red\">4. Explain the concept of gradient descent. How is it used in machine learning? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5fef83-b772-4f68-b2f4-5617304eb041",
   "metadata": {},
   "source": [
    "1. Gradiant descent is an optimization algorithm used in Machine Learning to minimize the cost function by iteratively adjusting parameters in the direction of negative gradiant.\n",
    "2. It starts with an initial guess for the parameters and then takes step forward the minimum of the cost function.\n",
    "3. Repeat until convergence (The process is repeated until a stopping criterion is met or cost function becomes very small )\n",
    "\n",
    "In machine learning, gradient descent is a fundamental technique used during the training phase of models. Here's how it's applied:\n",
    "\n",
    "Model Training:\n",
    "\n",
    "step 1 :- Given a dataset, a machine learning model's goal is to learn a set of parameters that minimize a cost function. This cost function measures the difference between the predicted values and the actual values.\n",
    "Calculate Loss.\n",
    "\n",
    "step 2 :- For each set of parameters, the model makes predictions on the training data, and the loss (or error) is calculated using the cost function.\n",
    "Compute Gradients.\n",
    "\n",
    "step 3 :- The gradients of the cost function with respect to the parameters are calculated. This indicates how the cost function changes with small changes in the parameters.\n",
    "Update Parameters.\n",
    "\n",
    "step 4 :-The parameters are adjusted in the direction that reduces the cost function. This is done by multiplying the gradients by the learning rate and subtracting them from the current parameters.\n",
    "Repeat Iteratively.\n",
    "\n",
    "Steps 2-4 are repeated for a predefined number of iterations or until the convergence criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ab460-149c-4690-9a28-27bc5b570911",
   "metadata": {},
   "source": [
    "<font color=\"red\"> 5. Describe the multiple linear regression model. How does it differ from simple linear regression? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e461678-b1ed-4258-adc1-a8567fd90ab6",
   "metadata": {},
   "source": [
    "Multiple Linear Regression is a statistical method used to model the relationship between a dependent variable and multiple independent variables.\n",
    "In Multiple Linear Regression algorithm is try to make a hyperplane with n-dimenssion coordinate by pass very close to data point.<br><br>\n",
    "y=β0 + β1x1 +β2x2 +.....+βnxn <br>\n",
    "\n",
    "Here, <br>\n",
    "β0 :- Intercept  <br>\n",
    "β1,β2,β3...βn = slope of (x1,x2,x3....xn) respectively or weight of features <br>\n",
    "x1,x2,x3...xn = Features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c12acd5-72fe-439c-b315-a60eedf8353e",
   "metadata": {},
   "source": [
    "Simple Linear Regression:-\n",
    "1. it involves only one independent variable\n",
    "2. Model complexity is less ( a linear relationship between two variables )\n",
    "3. Simple Linear Regression has a single predictor variable in its equation.\n",
    "4. Simple Linear Regression can be visualized as a straight line in a 2D plot.\n",
    "5. The slope represents the change in the dependent variable for a one-unit change in the independent variable.\n",
    "6. Simple Linear Regression is suitable when there is a single predictor variable that has a linear relationship with the target variable.\n",
    "\n",
    "Multiple Linear Regression :-\n",
    "1. It involves more than one independent variable.\n",
    "2. Model complexity is high ( a multiple linear relationship between the dependent and multiple independent variable )\n",
    "3. It has multiple predictor variable in the equation.\n",
    "4. It can be visualized as a hyperplane in multi-dimessional space it is harder to represent graphically.\n",
    "5. The coefficients represent the change in the dependent variable for a one-unit change in the respective independent variable, holding other variables constant.\n",
    "6. Multiple Linear Regression is used when there are multiple predictor variables that collectively influence the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd5eb31-591d-423c-ac4e-9da0a74652f5",
   "metadata": {},
   "source": [
    "<font color=\"red\"> 6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15571b59-6057-4664-969a-14f57c2b3bb2",
   "metadata": {},
   "source": [
    " Multicollinearity is a phenomena where two or more independent variables are highly correlated.\n",
    " This can cause problems in the regression analysis, as it can lead to unreliable and unstable estimates of the regression coefficients.\n",
    "1. When two or more predictor variables have a high correlation, it indicates that they are providing similar information to the regression model.\n",
    "2. Due to multicollinearity in linear regression , it face problem as we known β1 (coefficient of x1 ) show how many changes occur in target variable if we change change in x1 feature with considering other features as constant. but due to multicollinearity if we do change in one feature then it reflect to another feature. we cannot take other features as constant. This makes it difficult to determine the individual contribution of each predictor.\n",
    "3. It becomes challenging to interpret the importance of each predictor variable, as their effects are confounded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea7035-e201-4d7f-ac98-6475cb540755",
   "metadata": {},
   "source": [
    "We can detect multicollinearity by :-\n",
    "\n",
    "1. if you have Domain knowledge.\n",
    "2. By using scatter plot of each features with otherr features (if we see any pattern then it's show multicollinearity)\n",
    "3. correaltion matrix (by using heatmap if correlation is greater than 0.9 then we take it as multicollinearity)\n",
    "4. VIF (Variance Inflation Factor)\n",
    "\n",
    "we can handle it by doing :-\n",
    "\n",
    "1. Remove Highly Correlated Variables:- Remove one of the highly correlated variable.\n",
    "2. by using Lasso / ridge regression. (it shrink non important coefficient)\n",
    "3. Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33eb639-3bc6-4a1d-af9f-4800a96bcf57",
   "metadata": {},
   "source": [
    "<font color=\"red\"> 7. Describe the polynomial regression model. How is it different from linear regression? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32574ec4-02c9-4dca-ad6f-0efb9d5e32a8",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis that models the relationship between the independent variable (usually denoted as x) and the dependent variable (usually denoted as y) as an n-th degree polynomial. This allows us to capture more complex relationships between the variables compared to simple linear regression.\n",
    "y=β0 + β1x + β2x2 + β3x3 + ......+βnxn + E\n",
    "\n",
    "Here,\n",
    "y is dependent variable\n",
    "β0,β1,β2.. are the coefficients.\n",
    "x is independent variable \n",
    "n is degree of polynomial\n",
    "E is Error term \n",
    "\n",
    "In polynomial regression, x is raised to different powers (1, 2, 3, ..., n) to capture nonlinear relationships.\n",
    "Polynomial regression is more flexible than simple linear regression because it can model curves and other complex relationships.\n",
    "\n",
    "Difference from linear regression:-\n",
    "1. Model Complexity :- Linear regression models relationships as a straight line, which is a special case of polynomial regression with degree=1 or n=1 .\n",
    "    Polynomial regression allows for more complex curves, which can better fit data with non-linear patterns.\n",
    "\n",
    "2. Polynomial regression can lead to overfitting if a very high degree polynomial is used. This is because the model may try to fit the noise in the data.\n",
    "3. In polynomial regression, interpreting the coefficients becomes more complex as the degree of the polynomial increases.\n",
    "    but in linear regression it is easy.\n",
    "4. Polynomial regression is suitable for cases where the relationship between variables is clearly non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33c6a09-a912-4527-b7c1-fc495726d874",
   "metadata": {},
   "source": [
    "<font color=\"red\">8. What are the advantages and disadvantages of polynomial regression compared to linear \n",
    "regression? In what situations would you prefer to use polynomial regression. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad3692-4b62-4fa2-821d-429ffe94c22c",
   "metadata": {},
   "source": [
    "1. Polynomial regression can capture more complex relationships between variables compared to simple linear regression.\n",
    "2. In cases where the relationship is genuinely nonlinear, polynomial regression can provide a better fit to the data compared to linear regression.\n",
    "3. It allows for a wide range of possible relationships, making it suitable for a variety of data patterns.\n",
    "4. Using a high-degree polynomial can lead to overfitting.\n",
    "5. Higher-degree polynomials require more computational resources, making them slower to train and evaluate.\n",
    "\n",
    "we can prefer polynomial regression in following aspect:-\n",
    "1. Clearly Nonlinear Relationships\n",
    "2. Limited Data Range: In cases where the data spans a limited range, a polynomial model may be necessary to capture the underlying trend.\n",
    "3. Polynomial regression can be a useful tool for exploratory data analysis to uncover potential nonlinear trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df6bcc4-922c-4852-9bd8-cb83924a3994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
